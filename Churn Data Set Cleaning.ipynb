{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. QUESTION OR DECISION\n",
    "\n",
    "Q: What variables have the most effect on customer retention rates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. REQUIRED VARIABLES<br>\n",
    "1. CaseOrder: Integer. Examples: \"1\", \"56\"\n",
    "2. Customer_id: Object. Example: \"K409189\", \"F392029\"\n",
    "3. Interaction, UID: Object. Examples: \"aa90260b-4141-4a24-8e36-b04ce1f4f77b\", \"fb76459f-c047-4a9d-8af9-e0f7d4ac2524\"\n",
    "4. City: Object. Examples: \"Seattle\", \"Portland\"\n",
    "5. State: Object. Examples: \"WA\", \"OR\"\n",
    "6. County: Object. Examples: \"United States\", \"Canada\"\n",
    "7. Zip: Integer. Examples: \"98109\", \"29393\"\n",
    "8. Lat,Lng: Floats. Examples: \"46.3389\", \"-39.494\"\n",
    "9. Population: Integer. Examples: \"389383\", \"4832\"\n",
    "10. Area: Object. Examples: \"Urban\", \"Suburban\"\n",
    "11. TimeZone: Object. Examples: \"PST\", \"UTC\"\n",
    "12. Job: Object. Examples: \"Solicitor\", \"Chief Financial Officer\"\n",
    "13. Children: Float. Examples: \"1.0\", \"4.0\"\n",
    "14. Age: Float. Examples: \"68.0\", \"36.0\"\n",
    "15. Education: Object. Examples: \"Master's Degree\", \"Bachelor's Degree\"\n",
    "16. Employment: Object. Examples: \"Retired\", \"Part Time\"\n",
    "17. Income: Float. Examples: \"82092.00\", \"21003\"\n",
    "18. Marital: Object. Examples: \"Widowed\", \"Married\"\n",
    "19. Gender: Object. Examples: \"Male\", \"Female\"\n",
    "20. Churn: Object. Examples: \"Yes\", \"No\"\n",
    "21. Outage_sec_perweek: Float. Examples: \"12.02\", \"1.24\"\n",
    "22. Email: Integer. Examples: \"10\", \"2\"\n",
    "23. Contacts: Integer. Examples: \"0\", \"3\"\n",
    "24. Yearly_equip_failure: Integer. Examples: \"1\", \"0\"\n",
    "25. Techie: Object. Examples: \"Yes\", \"No\"\n",
    "26. Contract: Object. Examples: \"One year\", \"Three Years\"\n",
    "27. Port_modem: Object. Examples: \"Yes\", \"No\"\n",
    "28. Tablet: Object. Examples: \"Yes\", \"No\"\n",
    "29. InternetService: Object. Examples: \"Fiber Optic\", \"DSL\"\n",
    "30. Phone: Object. Examples: \"Yes\", \"No\"\n",
    "31. Multiple: Object. Examples: \"Yes\", \"No\"\n",
    "32. OnlineSecurity: Object. Examples: \"Yes\", \"No\"\n",
    "33. OnlineBackup: Object. Examples: \"Yes\", \"No\"\n",
    "34. DeviceProtection: Object. Examples: \"Yes\", \"No\"\n",
    "35. TechSupport: Object. Examples: \"Yes\", \"No\"\n",
    "36. StreamingTV: Object. Examples: \"Yes\", \"No\"\n",
    "37. StreamingMovies: Object. Examples: \"Yes\", \"No\"\n",
    "38. PaperlessBilling: Object. Examples: \"Yes\", \"No\"\n",
    "39. PaymentMethod: Object. Examples: \"Credit Card (automatic)\", \"Mailed Check\"\n",
    "40. Tenure: Float. Examples: \"4.382\", \"9.38\"\n",
    "41. MonthlyCharge: Float. Examples: \"281.28\", \"182.89\"\n",
    "42. Bandwidth_GB_Year: Float. Examples: \"920.94\", \"9399.123\"\n",
    "43. Item1: Integer. Examples: \"5\", \"3\"\n",
    "44. Item2: Integer. Examples: \"6\", \"5\"\n",
    "45. Item3: Integer. Examples: \"8\", \"4\"\n",
    "46. Item4: Integer. Examples: \"4\", \"2\"\n",
    "47. Item5: Integer. Examples: \"2\", \"8\"\n",
    "48. Item6: Integer. Examples: \"6\", \"6\"\n",
    "49. Item7: Integer. Examples: \"9\", \"1\"\n",
    "50. Item8: Integer. Examples: \"5\", \"5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the data set\n",
    "df = pd.read_csv('C:/Users/Conner/OneDrive/WGU/D206/churn_raw_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quickly take a look at the data set\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C1. PLAN TO FIND ANOMALIES\n",
    "1. Duplicates: The first step is to find if there are any duplicate rows present in the data by using the .duplicated() function. Because there are so many rows, using .duplicated().sum() makes it a little easier to see the total number of duplicate rows. From there, remove them using .drop_duplicates().\n",
    "2. Null: To find null values in the dataset, the function .isnull().sum() is used to display a list of the total null values in each column. For numerical columns replace missing values with the mean of each column. If there are large outliers present, it would be better to use the median. The remaining columns are True/False answers and will be replaced with the most frequent value (mode) found in them using the following functions: .fillna() and .value_counts(). [1] \n",
    "3. Outliers: Calculate the zscore in all of the numerical columns, then filter for scores that fall outside of -3 to 3. If the score isn't too far out of range, keep the value. For very extreme values, evaluate the context and determine if it should be kept. If not, replace with the mean of the column or possibly remove the row.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C2. JUSTIFICATION OF APPROACH\n",
    "1. Duplicates: Finding and removing duplicates is important because they can falsely skew your data if not dealt with. With this data set of customer information, there should be no duplicate customers because each person is unique. \n",
    "2. Null: In the context of the churn data, null values were likely the cause of customers simply not providing those values such as income or number of children. Another possibility would be an error caused by the system which could lead to entire rows of null values instead of just a few. In the case of full rows of missing data, entirely removing them is the best approach because there is no justification to keep them. For rows with one to a few missing values, it is important to look at what exactly is missing. With the churn data there were thousands of missing values and so just removing them was not the right approach because you would be losing a significant amount of important data. Instead, for numerical columns, I chose to replace null values with the mean of that column. That is one way to deal with them without heavily skewing or altering later analysis. With categorical data, I used the common approach of replacing nulls with the mode of the column. While this could potentially have a larger impact later during analysis, it seemed better than full removal due to the somewhat low number of values that had to be replaced.\n",
    "3. Outliers: The most important part of judging outliers in this data set and dealing with them was looking at the context. Just because you have a strong outlier doesn't necessarily mean something needs to be done with it. For example, you might see a value in the income column with a z score of 8. While this data point is many standard deviations away from the mean, the idea of a customer having a very high income is not out of the realm of possibility. What I found with this data set is that most if not all of the other columns with outliers could be fairly easily explained and therefore nothing was done with them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C3. JUSTIFICATION OF TOOLS<br>\n",
    "For this task, I chose to use the Python language for my cleaning and analysis. In general, it is fairly easy to use due to the massive amount of documentation and support found online as well as being a simple language that makes a lot of assumptions[8]. This simplifies your code and makes it easier to read/write.<br>\n",
    "There are also many libraries available that greatly simplify many tasks and calculations. The first and most important one is the pandas library, used for reading/writing the data and creating the data frame to be used for the rest of the analysis[3]. Second is the numpy library, used primarily for simplifying mathematical calculations[4]. For the outlier analysis, I used the stats package from the scipy library. In general, it removes the need for extensive/long calculations and can be implemented in simple functions[5]. PCA from sklearn allowed for very fast principal component analysis of the data set without having to understand or apply numerous mathematical functions[6]. Finally, pyplot from matplotlib was used to create simple graphs that are easy to quickly draw conclusions from[7].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C4. PROVIDE THE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the number of duplicated values\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the number of null values in each column\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace null values in Numerical columns with the mean. [1]\n",
    "df['Children'].fillna(df['Children'].mean(), inplace = True)\n",
    "df['Age'].fillna(df['Age'].mean(), inplace = True)\n",
    "df['Income'].fillna(df['Income'].mean(), inplace = True)\n",
    "df['Bandwidth_GB_Year'].fillna(df['Bandwidth_GB_Year'].mean(), inplace = True)\n",
    "df['Tenure'].fillna(df['Tenure'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Replace null values in Boolean/Categorical columns with the mode. [1]\n",
    "df['TechSupport'] = df['TechSupport'].fillna(df['TechSupport'].value_counts().index[0])\n",
    "df['PaymentMethod'] = df['PaymentMethod'].fillna(df['PaymentMethod'].value_counts().index[0])\n",
    "df['Phone'] = df['Phone'].fillna(df['Phone'].value_counts().index[0])\n",
    "df['Techie'] = df['Techie'].fillna(df['Techie'].value_counts().index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a brief look at basic stats of the numerical columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate zscore for each selected column. (Chantal D., et al., 2019)\n",
    "df['Population_zscore'] = stats.zscore(df['Population'])\n",
    "df['Children_zscore'] = stats.zscore(df['Children'])\n",
    "df['Age_zscore'] = stats.zscore(df['Age'])\n",
    "df['Income_zscore'] = stats.zscore(df['Income'])\n",
    "df['Outage_sec_perweek_zscore'] = stats.zscore(df['Outage_sec_perweek'])\n",
    "df['Email_zscore'] = stats.zscore(df['Email'])\n",
    "df['Contacts_zscore'] = stats.zscore(df['Contacts'])\n",
    "df['Yearly_equip_failure_zscore'] = stats.zscore(df['Yearly_equip_failure'])\n",
    "df['Tenure_zscore'] = stats.zscore(df['Tenure'])\n",
    "df['MonthlyCharge_zscore'] = stats.zscore(df['MonthlyCharge'])\n",
    "df['Bandwidth_GB_Year_zscore'] = stats.zscore(df['Bandwidth_GB_Year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify outliers. (Chantal D., et al., 2019)\n",
    "Population_outliers = df.query('Population_zscore > 3 | Population_zscore < -3')\n",
    "population_sort = Population_outliers.sort_values(['Population_zscore'], ascending = False)\n",
    "population_sort['Population_zscore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify outliers. (Chantal D., et al., 2019)\n",
    "children_outliers = df.query('Children_zscore > 3 | Children_zscore < -3')\n",
    "children_sort = children_outliers.sort_values(['Children_zscore'], ascending = False)\n",
    "children_sort['Children_zscore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify outliers. (Chantal D., et al., 2019)\n",
    "Age_outliers = df.query('Age_zscore > 3 | Age_zscore < -3')\n",
    "Age_sort = Age_outliers.sort_values(['Age_zscore'], ascending = False)\n",
    "Age_sort['Age_zscore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify outliers. (Chantal D., et al., 2019)\n",
    "Income_outliers = df.query('Income_zscore > 3 | Income_zscore < -3')\n",
    "Income_sort = Income_outliers.sort_values(['Income_zscore'], ascending = False)\n",
    "Income_sort['Income_zscore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify outliers. (Chantal D., et al., 2019)\n",
    "Outage_sec_perweek_outliers = df.query('Outage_sec_perweek_zscore > 3 | Outage_sec_perweek_zscore < -3')\n",
    "Outage_sec_perweek_sort = Outage_sec_perweek_outliers.sort_values(['Outage_sec_perweek_zscore'], ascending = False)\n",
    "Outage_sec_perweek_sort['Outage_sec_perweek_zscore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify outliers. (Chantal D., et al., 2019)\n",
    "Email_outliers = df.query('Email_zscore > 3 | Email_zscore < -3')\n",
    "Email_sort = Email_outliers.sort_values(['Email_zscore'], ascending = False)\n",
    "Email_sort['Email_zscore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify outliers. (Chantal D., et al., 2019)\n",
    "Contacts_outliers = df.query('Contacts_zscore > 3 | Contacts_zscore < -3')\n",
    "Contacts_sort = Contacts_outliers.sort_values(['Contacts_zscore'], ascending = False)\n",
    "Contacts_sort['Contacts_zscore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify outliers. (Chantal D., et al., 2019)\n",
    "Yearly_equip_failure_outliers = df.query('Yearly_equip_failure_zscore > 3 | Yearly_equip_failure_zscore < -3')\n",
    "Yearly_equip_failure_sort = Yearly_equip_failure_outliers.sort_values(['Yearly_equip_failure_zscore'], ascending = False)\n",
    "Yearly_equip_failure_sort['Yearly_equip_failure_zscore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify outliers. (Chantal D., et al., 2019)\n",
    "Tenure_outliers = df.query('Tenure_zscore > 3 | Tenure_zscore < -3')\n",
    "Tenure_sort = Tenure_outliers.sort_values(['Tenure_zscore'], ascending = False)\n",
    "Tenure_sort['Tenure_zscore'].head(n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify outliers. (Chantal D., et al., 2019)\n",
    "MonthlyCharge_outliers = df.query('MonthlyCharge_zscore > 3 | MonthlyCharge_zscore < -3')\n",
    "MonthlyCharge_sort = MonthlyCharge_outliers.sort_values(['MonthlyCharge_zscore'], ascending = False)\n",
    "MonthlyCharge_sort['MonthlyCharge_zscore'].head(n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify outliers. (Chantal D., et al., 2019)\n",
    "Bandwidth_GB_Year_outliers = df.query('Bandwidth_GB_Year_zscore > 3 | Bandwidth_GB_Year_zscore < -3')\n",
    "Bandwidth_GB_Year_sort = Bandwidth_GB_Year_outliers.sort_values(['Bandwidth_GB_Year_zscore'], ascending = False)\n",
    "Bandwidth_GB_Year_sort['Bandwidth_GB_Year_zscore'].head(n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D1. CLEANING FINDINGS\n",
    "- No duplicate rows were found in the entire data set.\n",
    "- Eight of the 50 columns contained null values, ranging from 931 to 2495 values in each. Removing them would significantly impact the data and any resulting analysis.\n",
    "- Outliers were only found in seven of the numerical columns (not including irrelevant ones such as latitude/longitude). Of those, all contained fewer than 5% outliers (total for the column). All outliers made logical sense and didn't seem to be from an error (e.g. data entry). One example would be from the income column, a value had a zscore of 8.91. While that is a significant outlier, if you look at the .describe() for that column you can see the max is not some outrageous salary that is not realistic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D2. JUSTIFICATION OF MITIGATION METHODS\n",
    "- Outliers: As mentioned previously, there were some outliers present in the data set, though not many. After looking at the context of each column and the values themselves, I decided not to do anything with them. Logically, outliers will be present in a data set and unless you have a good reason to remove them, you should generally keep them/not alter them.\n",
    "- Nulls: Due to the large number of null values found in both numerical and categorical columns, I opted to replace them with imputed values. For the numerical columns, null values were replaced with the mean of the column. This helps to preserve the overall structure of the data and shouldn't skew the data very much. For the categorical columns, the only other option (in the course textbook) besides completely removing the values is to impute using the mode. Thankfully for these columns, there were generally fewer than 1,000 null values and so replacing them with the mode would not have a large impact on later-drawn conclusions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D3. SUMMARY OF THE OUTCOMES\n",
    "- No duplicate rows were present so nothing had to be removed.\n",
    "- Null values present in numerical columns were imputed with the mean of the column, while null values in categorical columns were imputed with the mode.\n",
    "- Outliers were identified and evaluated on a per-column basis. None were removed or replaced due to a lack of reason/purpose.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D4. MITIGATION CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the code in section C4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D5. CLEAN DATA\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the cleaned dataframe to an Excel file.\n",
    "df.to_excel(\"Clean_churn_data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the attached data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D6. LIMITATIONS<br>\n",
    "Addressing the null values had several limitations. For the numerical columns, there were other ways to impute replacement values that could have different effects later on. Some of the common ones are using the median or mode, which would have the greatest effect on the mean of the column as well as the standard deviation. With the categorical columns, one of the only options is to impute using the mode. This can have a much greater effect, especially given that the columns in this data set were limited to True/False. Depending on the number of null values present, using the mode can have a significant skew on the data. Weighing that outcome against simply removing the row is something to take into consideration.<br>\n",
    "Another limitation present was simply the number of null values present in the data. Removing them all would have a significantly negative impact on any analysis, so any cleaning must use some sort of imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D7. IMPACT OF THE LIMITATIONS\n",
    "- Null values had to be imputed with other values rather than removed.\n",
    "- Any sort of replacing would have an impact on the overall data set, generally skewing the mean and standard deviation. \n",
    "- Removing and/or imputing data could potentially affect the relationships between variables and any conclusions drawn from them. An example might be analyzing churn rates and concluding the wrong variable is the most important in predicting future rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of numerical columns. (Chantal D., et al., 2019)\n",
    "df_numerical = df[['Zip', 'Lat', 'Lng', 'Population', 'Children', 'Age', 'Income', 'Email', 'Outage_sec_perweek', 'Contacts', 'Yearly_equip_failure', 'Tenure', 'MonthlyCharge', 'Bandwidth_GB_Year', 'item1', 'item2', 'item3', 'item4', 'item5', 'item6', 'item7', 'item8']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quickly check the new column set\n",
    "df_numerical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize numerical data. (Chantal D., et al., 2019)\n",
    "df_numerical_normalized = (df_numerical - df_numerical.mean())/df_numerical.std() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Chantal D., et al., 2019)\n",
    "pca = PCA(n_components = df_numerical.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert data set variables to components. (Chantal D., et al., 2019)\n",
    "pca.fit(df_numerical_normalized)\n",
    "df_numerical_pca = pd.DataFrame(pca.transform(df_numerical_normalized), columns = ['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14','PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot. (Chantal D., et al., 2019)\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract eigenvalues. (Chantal D., et al., 2019)\n",
    "cov_matrix = np.dot(df_numerical_normalized.T, df_numerical_normalized) / df_numerical.shape[0]\n",
    "eigenvalues = [np.dot(eigenvector.T, np.dot(cov_matrix, eigenvector)) for eigenvector in pca.components_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot eigenvalues. (Chantal D., et al., 2019)\n",
    "plt.plot(eigenvalues)\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('eigenvalue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display Principal component loadings. (Chantal D., et al., 2019)\n",
    "loadings = pd.DataFrame(pca.components_.T, \n",
    "columns = ['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14','PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22'],\n",
    "index = df_numerical.columns)\n",
    "loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E1. PRINCIPAL COMPONENTS\n",
    "\n",
    "- PC1: item1, item2, item3, item6, item7, item8\n",
    "- PC2: Zip, Lng\n",
    "- PC3: Yearly_equip_failure, Tenure, Bandwidth_GB_Year\n",
    "- PC4: item4, item5\n",
    "- PC5: Lat, Population, Children, Age, Income, Email, Outage_sec_perweek, Contacts, MonthlyCharge<br>\n",
    "\n",
    "Variables with the most weight:\n",
    "- Zip\n",
    "- Lat\n",
    "- Lng\n",
    "- Population\n",
    "- Tenure\n",
    "- Bandwidth_GB_year\n",
    "- item1 - item8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E2. CRITERIA USED<br>\n",
    "First the numerical column data was normalized, then the eigenvalues were calculated and printed on a plot. Looking at the plot, only 5 principal components met the criteria of being greater than 1. Next, the loadings of each of the 5 components was evaluated and the ones with the greatest load were sorted into their respective principal component groups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E3. BENEFITS<br>\n",
    "Looking at the grouping of the principal components, the company can get a general understanding of what variables have the greatest weight. In general, the location of the customer, their survey responses, and information about the customer themselves are the most important. Being able to narrow down a large number of variables from the beginning data set to only five makes it much easier to quickly/efficiently draw conclusions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F. VIDEO\n",
    "\n",
    "\n",
    "Link to video: https://wgu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=d45ee0e7-0376-4a5c-864b-ad700161ccde\n",
    "\n",
    "Also found in the submission links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### G. SOURCES FOR THIRD-PARTY CODE<br>\n",
    "1. \"A Comprehensive guide on handling Missing Values\", Reddy, M., https://medium.com/bycodegarage/a-comprehensive-guide-on-handling-missing-values-b1257a4866d1, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H. SOURCES  <br>\n",
    "2. Chantal D. Larose, Daniel T. Larose. \"Data Science Using Python and R\". Wiley; 2019. \n",
    "3. \"About pandas\", https://pandas.pydata.org/about/\n",
    "4. \"What is NumPy?\", https://numpy.org/doc/stable/user/whatisnumpy.html\n",
    "5. \"Scientific computing tools for Python\", https://www.scipy.org/about.html\n",
    "6. \"Scikit-learn: Machine Learning in Python\", Pedregosa et al., JMLR 12, pp. 2825-2830, 2011\n",
    "7.  J. D. Hunter, \"Matplotlib: A 2D Graphics Environment\", Computing in Science & Engineering, vol. 9, no. 3, pp. 90-95, 2007\n",
    "8. \"About\", https://www.python.org/about/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
